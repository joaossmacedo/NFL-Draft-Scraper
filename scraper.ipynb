{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38264bitnflscrapyconda25156b98acc14fb3a3b340cccbe41626",
   "display_name": "Python 3.8.2 64-bit ('nfl-scrapy': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_rows(soup):\n",
    "    table = soup.find(id='results')\n",
    "    tbody = table.find('tbody')\n",
    "    trs = tbody.find_all('tr')\n",
    "\n",
    "    return trs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapy_row(tr):\n",
    "    identity = None\n",
    "    body = None\n",
    "\n",
    "    td_year = tr.find('td', {'data-stat': 'year_id'})\n",
    "    td_round = tr.find('td', {'data-stat': 'draft_round'})\n",
    "    td_pick = tr.find('td', {'data-stat': 'draft_pick'})\n",
    "\n",
    "    td_name = tr.find('td', {'data-stat': 'player'})\n",
    "    td_position = tr.find('td', {'data-stat': 'pos'})\n",
    "    td_age = tr.find('td', {'data-stat': 'draft_age'})\n",
    "    td_first_team_ap = tr.find('td', {'data-stat': 'all_pros_first_team'})\n",
    "    td_pro_bowls = tr.find('td', {'data-stat': 'pro_bowls'})\n",
    "    td_team = tr.find('td', {'data-stat': 'team'})\n",
    "    td_av = tr.find('td', {'data-stat': 'career_av'})\n",
    "    \n",
    "    if td_year is None:\n",
    "        return None\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    data['year'] = td_year.find('a').text\n",
    "    data['round'] = td_round.text\n",
    "    data['pick'] = td_pick.text\n",
    "    data['player_name'] = td_name.text\n",
    "    data['position'] = td_position.text\n",
    "    data['age'] = td_age.text\n",
    "    data['first_team_ap'] = td_first_team_ap.text\n",
    "    data['pro_bowls'] = td_pro_bowls.text\n",
    "    data['team'] = td_team.text\n",
    "\n",
    "    av = td_av.text\n",
    "    if av == '':\n",
    "        av = '0'\n",
    "    data['av'] = av\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_url(soup):\n",
    "    next_btn = soup.find('a', {'class': 'button2 next'}, href=True)\n",
    "\n",
    "    if next_btn is None:\n",
    "        return None\n",
    "\n",
    "    return next_btn['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def save_player(player, writer):\n",
    "    print('async writting')\n",
    "    writer.writerow(player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(first_year=1970, last_year=2019, target='drafted_players'):\n",
    "    BASE_URL = 'https://www.pro-football-reference.com'\n",
    "    path = '/play-index/draft-finder.cgi?request=1&year_min=' + str(first_year) + '&year_max=' + str(last_year) + '&draft_slot_min=1&draft_slot_max=500&pick_type=overall&pos%5B%5D=qb&pos%5B%5D=rb&pos%5B%5D=wr&pos%5B%5D=te&pos%5B%5D=e&pos%5B%5D=t&pos%5B%5D=g&pos%5B%5D=c&pos%5B%5D=ol&pos%5B%5D=dt&pos%5B%5D=de&pos%5B%5D=dl&pos%5B%5D=ilb&pos%5B%5D=olb&pos%5B%5D=lb&pos%5B%5D=cb&pos%5B%5D=s&pos%5B%5D=db&pos%5B%5D=k&pos%5B%5D=p&conference=any&show=all&order_by=default'\n",
    "    page = 0\n",
    "\n",
    "    if os.path.isfile(target + '.csv'):\n",
    "        os.remove(target + '.csv')\n",
    "\n",
    "    with open(target + '.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['year', 'round', 'pick', 'player_name', 'position', 'age', \n",
    "                      'first_team_ap', 'pro_bowls', 'team', 'av']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        while True:\n",
    "            start = time.time()\n",
    "\n",
    "            soup = get_soup(BASE_URL + path)\n",
    "\n",
    "            trs = get_all_rows(soup)\n",
    "            for tr in trs:\n",
    "                player = scrapy_row(tr)\n",
    "                if player is None:\n",
    "                    continue\n",
    "\n",
    "                writer.writerow(player)\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            page += 1\n",
    "            print('Page ' + str(page) + ' took ' + str(round(end - start)) + 's')\n",
    "\n",
    "            path = get_next_url(soup)\n",
    "\n",
    "            if path is None:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Page 1 took 4s\nPage 2 took 4s\nPage 3 took 4s\nPage 4 took 3s\nPage 5 took 4s\nPage 6 took 8s\nPage 7 took 8s\nPage 8 took 8s\nPage 9 took 8s\nPage 10 took 13s\nPage 11 took 9s\nPage 12 took 8s\nPage 13 took 12s\nPage 14 took 9s\nPage 15 took 8s\nPage 16 took 8s\nPage 17 took 8s\nPage 18 took 7s\nPage 19 took 7s\nPage 20 took 9s\nPage 21 took 8s\nPage 22 took 4s\nPage 23 took 4s\nPage 24 took 4s\nPage 25 took 3s\nPage 26 took 3s\nPage 27 took 3s\nPage 28 took 3s\nPage 29 took 4s\nPage 30 took 4s\nPage 31 took 3s\nPage 32 took 3s\nPage 33 took 3s\nPage 34 took 3s\nPage 35 took 4s\nPage 36 took 3s\nPage 37 took 3s\nPage 38 took 4s\nPage 39 took 5s\nPage 40 took 4s\nPage 41 took 3s\nPage 42 took 3s\nPage 43 took 3s\nPage 44 took 4s\nPage 45 took 4s\nPage 46 took 4s\nPage 47 took 4s\nPage 48 took 3s\nPage 49 took 3s\nPage 50 took 3s\nPage 51 took 5s\nOverall time: 271s\n"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "get()\n",
    "\n",
    "end = time.time()\n",
    "print('Overall time: ' + str(round(end - start)) + 's')"
   ]
  }
 ]
}